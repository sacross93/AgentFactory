from langchain_ollama import OllamaLLM
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import AgentExecutor, create_react_agent
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any, Optional
import json
import streamlit as st
from langchain.memory import ConversationBufferMemory
import time
import base64

# í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ ì ìš©
st.set_page_config(
    page_title="ì œí”Œëª° ì¢…í•© ìƒë‹´ ë´‡",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS ì ìš©
st.markdown("""
<style>
    /* ì „ì²´ ì•± ìŠ¤íƒ€ì¼ */
    .main {
        background-color: #f8f9fa;
        color: #212529;
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    .stTitleContainer {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        margin-bottom: 1.5rem;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .stChatMessage {
        background-color: #ffffff;
        border-radius: 10px;
        padding: 0.5rem;
        margin-bottom: 0.5rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ */
    .stChatMessage[data-testid="stChatMessageUser"] {
        background-color: #e9f5ff;
    }
    
    /* ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ */
    .stChatMessage[data-testid="stChatMessageAssistant"] {
        background-color: #f0f7ff;
    }
    
    /* ì§„í–‰ ìƒíƒœ ì»¨í…Œì´ë„ˆ */
    .progress-container {
        background-color: #ffffff;
        border-radius: 10px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    
    /* ì§„í–‰ ìƒíƒœ ë°” */
    .stProgress > div > div {
        background-color: #4e8df5;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .css-1d391kg {
        background-color: #ffffff;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton>button {
        background-color: #4e8df5;
        color: white;
        border-radius: 5px;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: 500;
    }
    
    .stButton>button:hover {
        background-color: #3a7bd5;
    }
    
    /* í™•ì¥ íŒ¨ë„ ìŠ¤íƒ€ì¼ */
    .streamlit-expanderHeader {
        background-color: #f8f9fa;
        border-radius: 5px;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ê³ ì • ìŠ¤íƒ€ì¼ */
    .stChatInputContainer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background-color: #f8f9fa;
        padding: 1rem;
        z-index: 1000;
        border-top: 1px solid #e9ecef;
    }
    
    /* ì±„íŒ… ì˜ì—­ì— í•˜ë‹¨ ì—¬ë°± ì¶”ê°€ */
    .chat-container {
        margin-bottom: 70px;
    }
</style>
""", unsafe_allow_html=True)

# Ollama ëª¨ë¸ ì´ˆê¸°í™”
llm = OllamaLLM(
    model="exaone3.5:32b",
    base_url="http://192.168.110.102:11434"
)

# ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
search_tool = DuckDuckGoSearchResults()

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    original_question: str
    current_search_query: str
    search_results: List[str]
    collected_information: List[str]
    is_sufficient: bool
    suggested_queries: List[str]
    final_answer: Optional[str]
    iteration_count: int  # ë°˜ë³µ íšŸìˆ˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ í•„ë“œ ì¶”ê°€

# ê²€ìƒ‰ ì—ì´ì „íŠ¸ (Agent 1)
search_agent_prompt = PromptTemplate.from_template("""
You are a search agent. Your task is to search for information based on the given query.

Search Query: {input}

Use the following tool to search for information:
{tools}

Use the following format:
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the tool
Observation: the result of the tool
Thought: I now have the search results
Final Answer: Summarize the search results in a clear and concise way

{agent_scratchpad}
""")

search_agent = create_react_agent(llm, [search_tool], search_agent_prompt)
search_executor = AgentExecutor(
    agent=search_agent, 
    tools=[search_tool], 
    verbose=True,
    handle_parsing_errors=True  # íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€
)

# ê²€ì¦ ì—ì´ì „íŠ¸ (Agent 2)
verification_prompt = PromptTemplate.from_template("""
You are a verification agent. Your task is to determine if the collected information is sufficient to answer the original question.

Original Question: {original_question}
Collected Information:
{collected_information}

First, analyze the original question to identify:
1. The main topic
2. Specific requirements or constraints
3. What information would be needed to provide a comprehensive answer

Then, evaluate the collected information to determine if it addresses all aspects of the question.

First, provide a detailed analysis of the information collected and how it relates to the question.
Then, based on this analysis, determine if the information is sufficient.

Return your analysis in JSON format:
{{
    "verification_reason": "Detailed explanation of your analysis of the collected information in relation to the question",
    "is_sufficient": true/false
}}

Focus on providing a thorough analysis first, then make your determination about sufficiency.
""")

def verification_agent(state):
    collected_info = "\n".join(state["collected_information"])
    
    response = llm.invoke(verification_prompt.format(
        original_question=state["original_question"],
        collected_information=collected_info
    ))
    
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            return {
                "is_sufficient": result.get("is_sufficient", False),
                "verification_reason": result.get("verification_reason", "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            }
        else:
            # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "is_sufficient": False,
                "verification_reason": "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "is_sufficient": False,
            "verification_reason": "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

# ì¿¼ë¦¬ ì œì•ˆ ì—ì´ì „íŠ¸ (Agent 3)
query_suggestion_prompt = PromptTemplate.from_template("""
You are a query suggestion agent. Your task is to suggest new search queries to find more information.

Original Question: {original_question}
Current Search Query: {current_search_query}
Collected Information:
{collected_information}

What we still need to know:
1. What aspects of the original question are not yet answered?
2. What specific information is missing?

Suggest 1-3 new search queries in English that would help gather the missing information.
Return your suggestions in JSON format:
{{
    "suggested_queries": ["query1", "query2", "query3"]
}}
""")

def query_suggestion_agent(state):
    collected_info = "\n".join(state["collected_information"])
    response = llm.invoke(query_suggestion_prompt.format(
        original_question=state["original_question"],
        current_search_query=state["current_search_query"],
        collected_information=collected_info
    ))
    
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            return {"suggested_queries": result["suggested_queries"]}
        else:
            # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return {"suggested_queries": [
                f"AMD 5600G FPS in League of Legends", 
                f"League of Legends minimum requirements vs AMD 5600G"
            ]}
    except:
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {"suggested_queries": [
            f"AMD 5600G FPS in League of Legends", 
            f"League of Legends minimum requirements vs AMD 5600G"
        ]}

# ê¸°ì¡´ ChatMemory í´ë˜ìŠ¤ ëŒ€ì‹  Langchainì˜ Memory ì‚¬ìš©
def get_memory():
    if "langchain_memory" not in st.session_state:
        st.session_state.langchain_memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="question",
            output_key="answer"
        )
    return st.session_state.langchain_memory

# ìµœì¢… ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ì •ì˜
final_answer_prompt = PromptTemplate.from_template("""
You are a helpful AI assistant. Your task is to provide a comprehensive answer to the user's question based on the collected information.

Original Question: {original_question}
Collected Information:
{collected_information}

Chat History:
{chat_history}

Please provide a detailed, accurate, and helpful answer based on the collected information and chat history. 
Make sure to address all aspects of the question and provide specific details where available.
If the information is insufficient to answer any part of the question, acknowledge this limitation.

If this is a follow-up question to a previous conversation, make sure to consider the context of the previous messages.
For example, if the user previously asked about hardware requirements and now asks "what problems might occur?", 
you should understand they're referring to problems with the hardware discussed earlier.

Your answer should be well-structured, easy to understand, and directly relevant to the question.

IMPORTANT: Your final answer MUST be in Korean language, regardless of the language of the question or search results.
""")

# í•œê¸€ ë²ˆì—­ ì—ì´ì „íŠ¸ ì¶”ê°€
translation_prompt = PromptTemplate.from_template("""
You are a professional translator. Your task is to translate the given text into Korean.
The text might already contain some Korean, but ensure the entire response is in fluent, natural Korean.

Original text:
{text}

Please translate this text into Korean, maintaining the original meaning, tone, and technical accuracy.
If the text already contains Korean, make sure the entire response is in consistent, high-quality Korean.
"""
)

def translate_to_korean(text):
    """í…ìŠ¤íŠ¸ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜"""
    response = llm.invoke(translation_prompt.format(text=text))
    return response

# í•œê¸€ ë‹µë³€ ì²´í¬ í•¨ìˆ˜
def check_korean_response(response):
    """
    ì‘ë‹µì´ ì ì ˆí•œ í•œê¸€ë¡œ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    1. ì¤‘êµ­ì–´ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜
    2. í•œê¸€ì´ í•œ ê°œë„ ì—†ìœ¼ë©´
    ë²ˆì—­ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨
    """
    # ì¤‘êµ­ì–´ ë¬¸ì ë²”ìœ„ ì²´í¬ (ê°„ì²´ ë° ë²ˆì²´)
    has_chinese = any(0x4E00 <= ord(char) <= 0x9FFF for char in response)
    
    # í•œê¸€ ë¬¸ì ì²´í¬
    has_korean = any(0xAC00 <= ord(char) <= 0xD7A3 for char in response)
    
    # ì¤‘êµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ í•œê¸€ì´ ì—†ìœ¼ë©´ ë²ˆì—­ í•„ìš”
    needs_translation = has_chinese or not has_korean
    
    return not needs_translation, needs_translation

# ìµœì¢… ë‹µë³€ ìƒì„± í•¨ìˆ˜ ìˆ˜ì •
def final_answer_agent(state):
    collected_info = "\n".join(state["collected_information"])
    chat_history = state.get("chat_history", "")
    
    response = llm.invoke(final_answer_prompt.format(
        original_question=state["original_question"],
        collected_information=collected_info,
        chat_history=chat_history
    ))
    
    # í•œê¸€ ì‘ë‹µ ì²´í¬
    is_korean, needs_translation = check_korean_response(response)
    
    # ë²ˆì—­ì´ í•„ìš”í•œ ê²½ìš°
    if needs_translation:
        translated_response = translate_to_korean(response)
        return {"final_answer": translated_response, "was_translated": True}
    
    return {"final_answer": response, "was_translated": False}

# ê²€ìƒ‰ ë…¸ë“œ
def search_node(state):
    # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
    state["iteration_count"] += 1
    
    result = search_executor.invoke({
        "input": state["current_search_query"],
        "tools": [search_tool],
        "tool_names": ["DuckDuckGoSearchResults"],
        "agent_scratchpad": ""
    })
    
    search_results = result["output"]
    state["search_results"].append(search_results)
    state["collected_information"].append(f"Search for '{state['current_search_query']}': {search_results}")
    
    return state

# ë‹¤ìŒ ì¿¼ë¦¬ ì„ íƒ ë…¸ë“œ
def select_next_query(state):
    if state["suggested_queries"]:
        state["current_search_query"] = state["suggested_queries"][0]
        state["suggested_queries"] = state["suggested_queries"][1:]
    return state

# ë¼ìš°í„° í•¨ìˆ˜
def router(state):
    # ìµœëŒ€ 5ë²ˆ ë°˜ë³µ ì œí•œ ì¶”ê°€
    if state["iteration_count"] >= 5:
        return "generate_answer"
    
    if state["is_sufficient"]:
        return "generate_answer"
    elif state["suggested_queries"]:
        return "select_next_query"
    else:
        return "suggest_queries"

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("search", search_node)
workflow.add_node("verify", verification_agent)
workflow.add_node("suggest_queries", query_suggestion_agent)
workflow.add_node("select_next_query", select_next_query)
workflow.add_node("generate_answer", final_answer_agent)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge("search", "verify")
# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ - verify ë…¸ë“œì—ì„œ ë¼ìš°í„° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
workflow.add_conditional_edges(
    "verify",
    router,
    {
        "generate_answer": "generate_answer",
        "select_next_query": "select_next_query",
        "suggest_queries": "suggest_queries"
    }
)
workflow.add_edge("suggest_queries", "select_next_query")
workflow.add_edge("select_next_query", "search")
workflow.add_edge("generate_answer", END)

# ì‹œì‘ì  ì¶”ê°€
workflow.set_entry_point("search")

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ì¿¼ë¦¬ ìµœì í™” ì—ì´ì „íŠ¸ ê°œì„ 
query_optimization_prompt = PromptTemplate.from_template("""
You are a search query optimization agent. Your task is to convert a user's question into effective search queries.

User Question: {question}
Chat History:
{chat_history}

First, analyze the question and chat history to identify:
1. Key topics and entities
2. Technical terms
3. Specific requirements or constraints
4. Context from previous conversation

IMPORTANT: 
- If this is a follow-up question, use the context from the chat history to create more specific queries.
- If the question is not in English, translate the key concepts to English for better search results.

Then, create 1-3 effective search queries in English that would help find relevant information.
The queries should be concise, specific, and use appropriate technical terms.

For example:
- If the previous conversation was about "AMD 5600g without GPU for League of Legends" and the new question is 
  "what problems might occur?", create queries like:
  "AMD 5600G integrated graphics problems running games"
  "Issues playing games without dedicated GPU on AMD 5600G"

Return your analysis and queries in JSON format:
{{
    "analysis": "Brief analysis of the question and its context",
    "search_queries": ["query1", "query2", "query3"]
}}
""")

def query_optimization_agent(question, memory=None):
    chat_history = ""
    if memory:
        memory_variables = memory.load_memory_variables({})
        chat_history = memory_variables.get("chat_history", "")
    
    response = llm.invoke(query_optimization_prompt.format(
        question=question,
        chat_history=chat_history
    ))
    
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            return result["search_queries"]
        else:
            # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return [f"AMD 5600G League of Legends performance", 
                   f"Can League of Legends run on AMD 5600G without dedicated GPU"]
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return [f"AMD 5600G League of Legends performance", 
               f"Can League of Legends run on AMD 5600G without dedicated GPU"]

# ê²€ìƒ‰ ë„êµ¬ ìˆ˜ì • - ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš© ë³´ì¥
def search_with_progress(state):
    state["iteration_count"] += 1
    
    # í˜„ì¬ ì¿¼ë¦¬ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸í•˜ê³  ì˜ì–´ë¡œ ë³€í™˜
    current_query = state["current_search_query"]
    if any(ord(char) > 127 for char in current_query):
        english_query = f"AMD 5600G League of Legends performance {state['iteration_count']}"
        log_message = f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬ (ì›ë³¸): {current_query}"
        process_logs.append(log_message)
        with progress_container:
            st.write(log_message)
            
        log_message = f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ì–´ë¡œ ë³€í™˜): {english_query}"
        process_logs.append(log_message)
        with progress_container:
            st.write(log_message)
            
        current_query = english_query
    else:
        log_message = f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬: {current_query}"
        process_logs.append(log_message)
        with progress_container:
            st.write(log_message)
    
    # ì˜ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹¤í–‰
    result = search_executor.invoke({
        "input": current_query,
        "tools": [search_tool],
        "tool_names": ["DuckDuckGoSearchResults"],
        "agent_scratchpad": ""
    })
    
    search_results = result["output"]
    log_message = f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {search_results[:200]}..."
    process_logs.append(log_message)
    with progress_container:
        st.write(log_message)
    
    state["search_results"].append(search_results)
    state["collected_information"].append(f"Search for '{current_query}': {search_results}")
    
    return state

# ì‹¤í–‰ í•¨ìˆ˜ ìˆ˜ì •
def run_agent_workflow(question, memory=None):
    chat_history = ""
    if memory:
        # Langchain ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        memory_variables = memory.load_memory_variables({})
        chat_history = memory_variables.get("chat_history", "")
    
    # ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± (ëŒ€í™” ê¸°ë¡ ì „ë‹¬)
    optimized_queries = query_optimization_agent(question, memory)
    
    # ì˜ì–´ ì¿¼ë¦¬ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
    english_queries = []
    for query in optimized_queries:
        if any(ord(char) > 127 for char in query):  # í•œêµ­ì–´ ë¬¸ì í¬í•¨ ì—¬ë¶€ í™•ì¸
            # ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
            english_queries.append(f"AMD 5600G League of Legends performance")
        else:
            english_queries.append(query)
    
    if not english_queries:
        english_queries = [
            "AMD 5600G League of Legends performance",
            "Can League of Legends run on AMD 5600G without dedicated GPU"
        ]
    
    initial_query = english_queries[0]
    
    initial_state = {
        "original_question": question,
        "current_search_query": initial_query,  # ìµœì í™”ëœ ì²« ë²ˆì§¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
        "search_results": [],
        "collected_information": [],
        "is_sufficient": False,
        "suggested_queries": english_queries[1:] if len(english_queries) > 1 else [],  # ë‚˜ë¨¸ì§€ ì¿¼ë¦¬ë“¤ì„ ì œì•ˆ ì¿¼ë¦¬ë¡œ ì„¤ì •
        "final_answer": None,
        "iteration_count": 0,
        "chat_history": chat_history
    }
    
    result = graph.invoke(initial_state)
    was_translated = result.get("was_translated", False)
    
    # ë²ˆì—­ ì—¬ë¶€ì— ë”°ë¥¸ ë¡œê·¸ ì¶”ê°€
    if was_translated:
        process_logs.append("ğŸ”„ ì›ë³¸ ë‹µë³€ì´ í•œê¸€ì´ ì•„ë‹ˆì–´ì„œ ë²ˆì—­ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.")
    
    return result["final_answer"]

# Streamlit ì•± ìˆ˜ì •
def main():
    # ì „ì—­ ë³€ìˆ˜ llmì„ í•¨ìˆ˜ ë‚´ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„ ì–¸
    global llm
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "current_question" not in st.session_state:
        st.session_state.current_question = None
    if "process_logs" not in st.session_state:
        st.session_state.process_logs = []
    
    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    memory = get_memory()
    
    # í—¤ë” í‘œì‹œ
    st.title("ğŸ” ì œí”Œëª° ì¢…í•© ìƒë‹´ ë´‡")
    st.markdown("ì»´í“¨í„° ë¶€í’ˆ, ê²Œì„ ì‚¬ì–‘, í˜¸í™˜ì„± ë“±ì— ê´€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ ë“œë¦½ë‹ˆë‹¤.")
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.title("ì„¤ì • ë° ì˜µì…˜")
    st.sidebar.markdown("---")
    
    if st.sidebar.button("ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°"):
        memory.clear()
        st.session_state.messages = []
        st.session_state.process_logs = []
        st.rerun()
    
    # ê²€ìƒ‰ ì„¤ì • ì„¹ì…˜
    st.sidebar.markdown("## ê²€ìƒ‰ ì„¤ì •")
    max_searches = st.sidebar.slider("ìµœëŒ€ ê²€ìƒ‰ íšŸìˆ˜", min_value=1, max_value=10, value=5, step=1)
    show_search_process = st.sidebar.checkbox("ê²€ìƒ‰ ê³¼ì • ìë™ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸°", value=False)
    
    # ëª¨ë¸ ì„¤ì • ì„¹ì…˜
    st.sidebar.markdown("## ëª¨ë¸ ì„¤ì •")
    model_options = ["exaone3.5:32b", "qwen2.5:32b", "deepseek-r1:32b"]
    selected_model = st.sidebar.selectbox("AI ëª¨ë¸ ì„ íƒ", model_options, index=0)
    
    # ëª¨ë¸ ì„¤ì • ì ìš©
    if selected_model != "exaone3.5:32b":
        # ëª¨ë¸ ë³€ê²½ ë¡œì§
        llm = OllamaLLM(
            model=selected_model,
            base_url="http://192.168.110.102:11434"
        )
    
    # ëŒ€í™” ìŠ¤íƒ€ì¼ ì„¤ì •
    st.sidebar.markdown("## ëŒ€í™” ìŠ¤íƒ€ì¼")
    conversation_style = st.sidebar.selectbox(
        "ë‹µë³€ ìŠ¤íƒ€ì¼",
        ["í‘œì¤€", "ìƒì„¸í•œ ì„¤ëª…", "ê°„ê²°í•œ ìš”ì•½", "ì „ë¬¸ê°€ ìˆ˜ì¤€"],
        index=0
    )
    
    # ìì£¼ ë¬»ëŠ” ì§ˆë¬¸
    st.sidebar.markdown("## ìì£¼ ë¬»ëŠ” ì§ˆë¬¸")
    faq_questions = [
        "ì œí”Œëª°ì—ì„œ ê°€ì¥ ì¸ê¸°ìˆëŠ” CPUëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ê²Œì´ë° PC êµ¬ì„± ì¶”ì²œí•´ì£¼ì„¸ìš”",
        "ê·¸ë˜í”½ì¹´ë“œ ì—†ì´ ê²Œì„ì„ í•  ìˆ˜ ìˆë‚˜ìš”?",
        "RAMì€ ì–¼ë§ˆë‚˜ í•„ìš”í•œê°€ìš”?"
    ]
    selected_faq = st.sidebar.selectbox("ì§ˆë¬¸ ì„ íƒ", ["ì„ íƒí•˜ì„¸ìš”..."] + faq_questions)
    if selected_faq != "ì„ íƒí•˜ì„¸ìš”...":
        # ì„ íƒëœ FAQ ì§ˆë¬¸ì„ ì…ë ¥ì°½ì— ìë™ìœ¼ë¡œ ì„¤ì •
        st.session_state.current_question = selected_faq
        st.rerun()

    # ëŒ€í™” ë‚´ë³´ë‚´ê¸°/ê°€ì ¸ì˜¤ê¸° ê¸°ëŠ¥
    st.sidebar.markdown("## ëŒ€í™” ê´€ë¦¬")
    if st.sidebar.button("ëŒ€í™” ë‚´ë³´ë‚´ê¸°"):
        # ëŒ€í™” ë‚´ìš©ì„ JSONìœ¼ë¡œ ë³€í™˜
        chat_export = json.dumps(st.session_state.messages, ensure_ascii=False)
        # ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„±
        b64 = base64.b64encode(chat_export.encode()).decode()
        href = f'<a href="data:file/json;base64,{b64}" download="chat_export.json">ëŒ€í™” ë‚´ìš© ë‹¤ìš´ë¡œë“œ</a>'
        st.sidebar.markdown(href, unsafe_allow_html=True)

    uploaded_file = st.sidebar.file_uploader("ëŒ€í™” ê°€ì ¸ì˜¤ê¸°", type="json")
    if uploaded_file is not None:
        try:
            imported_messages = json.loads(uploaded_file.read())
            st.session_state.messages = imported_messages
            st.sidebar.success("ëŒ€í™” ë‚´ìš©ì„ ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤!")
            st.rerun()
        except Exception as e:
            st.sidebar.error(f"íŒŒì¼ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")

    # ì •ë³´ ì„¹ì…˜
    st.sidebar.markdown("---")
    st.sidebar.markdown("## ì •ë³´")
    st.sidebar.markdown("""
ğŸ’¬ **ì œí”Œëª° ì¢…í•© ìƒë‹´ ë´‡**  
ğŸ”„ ë²„ì „: 1.0.0  
ğŸ‘¨â€ğŸ’» ê°œë°œ: AI ì—°êµ¬íŒ€ ê¹€ì§„ì˜ ì±…ì„
""")

    # í”¼ë“œë°± ì„¹ì…˜
    st.sidebar.markdown("## í”¼ë“œë°±")
    feedback = st.sidebar.text_area("ì˜ê²¬ì´ë‚˜ ë²„ê·¸ ì œë³´", height=100)
    if st.sidebar.button("ì œì¶œ"):
        if feedback.strip():  # ë¹ˆ í”¼ë“œë°±ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ
            with open("feedback_log.txt", "a") as f:
                f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')}: {feedback}\n")
            st.sidebar.success("í”¼ë“œë°±ì´ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
        else:
            st.sidebar.warning("í”¼ë“œë°±ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    # ëŒ€í™” ì»¨í…Œì´ë„ˆ ìƒì„± (í•˜ë‹¨ ì—¬ë°± ì¶”ê°€)
    chat_container = st.container()
    
    # ëŒ€í™” ì…ë ¥ì°½ (í•­ìƒ ë§¨ ì•„ë˜ì— ìœ„ì¹˜)
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    
    # ëŒ€í™” ê¸°ë¡ í‘œì‹œ (ëŒ€í™” ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
    with chat_container:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])
        st.markdown('</div>', unsafe_allow_html=True)
    
    if user_input and not st.session_state.processing:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": user_input})
        with chat_container:
            with st.chat_message("user"):
                st.write(user_input)
        
        # ì²˜ë¦¬ ì¤‘ ìƒíƒœë¡œ ì„¤ì •
        st.session_state.processing = True
        st.session_state.current_question = user_input
        st.session_state.process_logs = []  # ë¡œê·¸ ì´ˆê¸°í™”
        
        # ì²˜ë¦¬ ì‹œì‘ì„ ìœ„í•œ í˜ì´ì§€ ê°±ì‹ 
        st.rerun()
    
    # ì²˜ë¦¬ ì¤‘ì´ê³  í˜„ì¬ ì§ˆë¬¸ì´ ìˆëŠ” ê²½ìš°
    if st.session_state.processing and st.session_state.current_question:
        with chat_container:
            with st.chat_message("assistant"):
                # ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
                progress_container = st.container()
                
                with progress_container:
                    # ì§„í–‰ ìƒíƒœ í‘œì‹œ ì»´í¬ë„ŒíŠ¸
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
                    def update_progress(progress, status):
                        progress_bar.progress(progress)
                        status_text.markdown(f"**{status}**")
                        st.session_state.process_logs.append(status)
                        time.sleep(0.1)  # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì§§ì€ ì§€ì—°
                    
                    # ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                    update_progress(0.1, "ğŸ” ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì¤‘...")
                    
                    optimized_queries = query_optimization_agent(st.session_state.current_question, memory)
                    
                    # ì˜ì–´ ì¿¼ë¦¬ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
                    english_queries = []
                    for query in optimized_queries:
                        if any(ord(char) > 127 for char in query):  # í•œêµ­ì–´ ë¬¸ì í¬í•¨ ì—¬ë¶€ í™•ì¸
                            # ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
                            english_queries.append(f"AMD 5600G League of Legends performance")
                        else:
                            english_queries.append(query)
                    
                    if not english_queries:
                        english_queries = [
                            "AMD 5600G League of Legends performance",
                            "Can League of Legends run on AMD 5600G without dedicated GPU"
                        ]
                    
                    update_progress(0.2, f"ğŸ“ ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {', '.join(english_queries)}")
                    
                    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                    memory_variables = memory.load_memory_variables({})
                    chat_history = memory_variables.get("chat_history", "")
                    
                    initial_state = {
                        "original_question": st.session_state.current_question,
                        "current_search_query": english_queries[0],  # ìµœì í™”ëœ ì²« ë²ˆì§¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
                        "search_results": [],
                        "collected_information": [],
                        "is_sufficient": False,
                        "suggested_queries": english_queries[1:] if len(english_queries) > 1 else [],  # ë‚˜ë¨¸ì§€ ì¿¼ë¦¬ë“¤ì„ ì œì•ˆ ì¿¼ë¦¬ë¡œ ì„¤ì •
                        "final_answer": None,
                        "iteration_count": 0,
                        "chat_history": chat_history
                    }
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    sufficient_info = False
                    for i, query in enumerate(english_queries[:min(5, len(english_queries))]):
                        progress_value = 0.2 + (i * 0.15)  # ê° ê²€ìƒ‰ë§ˆë‹¤ ì§„í–‰ë¥  ì¦ê°€
                        
                        update_progress(progress_value, f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘: {query}")
                        
                        # ê²€ìƒ‰ ì‹¤í–‰
                        result = search_executor.invoke({
                            "input": query,
                            "tools": [search_tool],
                            "tool_names": ["DuckDuckGoSearchResults"],
                            "agent_scratchpad": ""
                        })
                        
                        search_results = result["output"]
                        truncated_results = search_results[:200] + "..." if len(search_results) > 200 else search_results
                        
                        update_progress(progress_value + 0.05, f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘...")
                        st.session_state.process_logs.append(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {truncated_results}")
                        
                        initial_state["search_results"].append(search_results)
                        initial_state["collected_information"].append(f"Search for '{query}': {search_results}")
                        
                        # ê²€ì¦ ë‹¨ê³„
                        update_progress(progress_value + 0.1, "âœ… ì •ë³´ ê²€ì¦ ì¤‘...")
                        
                        # ê²€ì¦ ì—ì´ì „íŠ¸ í˜¸ì¶œ
                        collected_info = "\n".join(initial_state["collected_information"])
                        verification_response = llm.invoke(verification_prompt.format(
                            original_question=initial_state["original_question"],
                            collected_information=collected_info
                        ))
                        
                        try:
                            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                            import re
                            json_match = re.search(r'\{.*\}', verification_response, re.DOTALL)
                            if json_match:
                                json_str = json_match.group(0)
                                verification_result = json.loads(json_str)
                                verification_reason = verification_result.get("verification_reason", "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                is_sufficient = verification_result.get("is_sufficient", False)
                            else:
                                verification_reason = "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                is_sufficient = False
                        except Exception as e:
                            print(f"Error parsing JSON: {e}")
                            verification_reason = "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            is_sufficient = False
                        
                        # ê²€ì¦ ê²°ê³¼ ë¡œê·¸ì— ì¶”ê°€
                        st.session_state.process_logs.append(f"ğŸ’¡ ê²€ì¦ ë¶„ì„: {verification_reason}")
                        
                        if is_sufficient:
                            update_progress(progress_value + 0.15, "ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ë¨ âœ…")
                            st.session_state.process_logs.append("ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ë¨ âœ…")
                            sufficient_info = True
                            break
                        else:
                            update_progress(progress_value + 0.15, "ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶”ê°€ ì •ë³´ í•„ìš” âŒ")
                            st.session_state.process_logs.append("ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶”ê°€ ì •ë³´ í•„ìš” âŒ")
                    
                    # ìµœì¢… ë‹µë³€ ìƒì„±
                    update_progress(0.9, "ğŸ“ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
                    
                    # ëŒ€í™” ìŠ¤íƒ€ì¼ ì ìš©
                    style_instruction = ""
                    if conversation_style == "ìƒì„¸í•œ ì„¤ëª…":
                        style_instruction = "ë‹µë³€ì€ ë§¤ìš° ìƒì„¸í•˜ê³  êµìœ¡ì ì¸ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê°œë…ì„ ìì„¸íˆ ì„¤ëª…í•˜ê³  ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì„¸ìš”."
                    elif conversation_style == "ê°„ê²°í•œ ìš”ì•½":
                        style_instruction = "ë‹µë³€ì€ ê°„ê²°í•˜ê³  í•µì‹¬ë§Œ ë‹´ì€ ìš”ì•½ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”. ë¶ˆí•„ìš”í•œ ì„¸ë¶€ ì‚¬í•­ì€ ìƒëµí•˜ê³  í•µì‹¬ ì •ë³´ë§Œ ì œê³µí•˜ì„¸ìš”."
                    elif conversation_style == "ì „ë¬¸ê°€ ìˆ˜ì¤€":
                        style_instruction = "ë‹µë³€ì€ í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ê°€ ì‘ì„±í•œ ê²ƒì²˜ëŸ¼ ì „ë¬¸ ìš©ì–´ì™€ ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ í¬í•¨í•˜ì„¸ìš”."
                    
                    # ìŠ¤íƒ€ì¼ ì§€ì‹œê°€ ìˆëŠ” ê²½ìš° í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
                    if style_instruction:
                        custom_final_answer_prompt = PromptTemplate.from_template(f"""
                        You are a helpful AI assistant. Your task is to provide a comprehensive answer to the user's question based on the collected information.

                        Original Question: {{original_question}}
                        Collected Information:
                        {{collected_information}}

                        Chat History:
                        {{chat_history}}

                        {style_instruction}

                        Please provide a detailed, accurate, and helpful answer based on the collected information and chat history. 
                        Make sure to address all aspects of the question and provide specific details where available.
                        If the information is insufficient to answer any part of the question, acknowledge this limitation.

                        If this is a follow-up question to a previous conversation, make sure to consider the context of the previous messages.
                        For example, if the user previously asked about hardware requirements and now asks "what problems might occur?", 
                        you should understand they're referring to problems with the hardware discussed earlier.

                        Your answer should be well-structured, easy to understand, and directly relevant to the question.

                        IMPORTANT: Your final answer MUST be in Korean language, regardless of the language of the question or search results.
                        """)
                        
                        # ìµœì¢… ë‹µë³€ ìƒì„± ì‹œ ìˆ˜ì •ëœ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                        final_answer_response = llm.invoke(custom_final_answer_prompt.format(
                            original_question=initial_state["original_question"],
                            collected_information="\n".join(initial_state["collected_information"]),
                            chat_history=initial_state["chat_history"]
                        ))
                    else:
                        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
                        final_answer_response = llm.invoke(final_answer_prompt.format(
                            original_question=initial_state["original_question"],
                            collected_information="\n".join(initial_state["collected_information"]),
                            chat_history=initial_state["chat_history"]
                        ))
                    
                    answer = final_answer_response
                    
                    # ì§„í–‰ ì™„ë£Œ
                    update_progress(1.0, "âœ¨ ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                    time.sleep(0.5)  # ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ì ì‹œ í‘œì‹œ
                    
                    # ì§„í–‰ ìƒí™© ì»¨í…Œì´ë„ˆ ë¹„ìš°ê¸°
                    progress_container.empty()
                
                # ìµœì¢… ë‹µë³€ í‘œì‹œ (ì¹´ë“œ í˜•íƒœë¡œ)
                # st.markdown("""
                # <div style="background-color: #f0f7ff; padding: 1.5rem; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                # """, unsafe_allow_html=True)
                st.write(answer)
                # st.markdown("</div>", unsafe_allow_html=True)
                
                # ìƒì„± ê³¼ì •ì„ í† ê¸€ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
                with st.expander("ğŸ“Š ë‹µë³€ ìƒì„± ê³¼ì • ë³´ê¸°", expanded=False):
                    for log in st.session_state.process_logs:
                        st.write(log)
                
                # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
                memory.save_context({"question": st.session_state.current_question}, {"answer": answer})
                
                # ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ì„¤ì •
                st.session_state.processing = False
                st.session_state.current_question = None

                # ë²ˆì—­ ì—¬ë¶€ í‘œì‹œ
                if "was_translated" in result and result["was_translated"]:
                    st.info("âš ï¸ ì´ ë‹µë³€ì€ ìë™ìœ¼ë¡œ í•œê¸€ë¡œ ë²ˆì—­ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
