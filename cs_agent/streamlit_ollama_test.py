from langchain_ollama import OllamaLLM
from langchain_community.tools import DuckDuckGoSearchResults
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import AgentExecutor, create_react_agent
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any, Optional
import json
import streamlit as st
from langchain.memory import ConversationBufferMemory
import time

# í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ ì ìš©
st.set_page_config(
    page_title="ì œí”Œëª° ì¢…í•© ìƒë‹´ ë´‡",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ì»¤ìŠ¤í…€ CSS ì ìš©
st.markdown("""
<style>
    /* ì „ì²´ ì•± ìŠ¤íƒ€ì¼ */
    .main {
        background-color: #f8f9fa;
        color: #212529;
    }
    
    /* í—¤ë” ìŠ¤íƒ€ì¼ */
    .stTitleContainer {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        margin-bottom: 1.5rem;
    }
    
    /* ì±„íŒ… ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
    .stChatMessage {
        background-color: #ffffff;
        border-radius: 10px;
        padding: 0.5rem;
        margin-bottom: 0.5rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    
    /* ì‚¬ìš©ì ë©”ì‹œì§€ */
    .stChatMessage[data-testid="stChatMessageUser"] {
        background-color: #e9f5ff;
    }
    
    /* ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ */
    .stChatMessage[data-testid="stChatMessageAssistant"] {
        background-color: #f0f7ff;
    }
    
    /* ì§„í–‰ ìƒíƒœ ì»¨í…Œì´ë„ˆ */
    .progress-container {
        background-color: #ffffff;
        border-radius: 10px;
        padding: 1rem;
        margin-bottom: 1rem;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    
    /* ì§„í–‰ ìƒíƒœ ë°” */
    .stProgress > div > div {
        background-color: #4e8df5;
    }
    
    /* ì‚¬ì´ë“œë°” ìŠ¤íƒ€ì¼ */
    .css-1d391kg {
        background-color: #ffffff;
    }
    
    /* ë²„íŠ¼ ìŠ¤íƒ€ì¼ */
    .stButton>button {
        background-color: #4e8df5;
        color: white;
        border-radius: 5px;
        border: none;
        padding: 0.5rem 1rem;
        font-weight: 500;
    }
    
    .stButton>button:hover {
        background-color: #3a7bd5;
    }
    
    /* í™•ì¥ íŒ¨ë„ ìŠ¤íƒ€ì¼ */
    .streamlit-expanderHeader {
        background-color: #f8f9fa;
        border-radius: 5px;
    }
    
    /* ì±„íŒ… ì…ë ¥ì°½ ê³ ì • ìŠ¤íƒ€ì¼ */
    .stChatInputContainer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background-color: #f8f9fa;
        padding: 1rem;
        z-index: 1000;
        border-top: 1px solid #e9ecef;
    }
    
    /* ì±„íŒ… ì˜ì—­ì— í•˜ë‹¨ ì—¬ë°± ì¶”ê°€ */
    .chat-container {
        margin-bottom: 70px;
    }
</style>
""", unsafe_allow_html=True)

# Ollama ëª¨ë¸ ì´ˆê¸°í™”
llm = OllamaLLM(
    model="exaone3.5:32b",
    base_url="http://192.168.110.102:11434"
)

# ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
search_tool = DuckDuckGoSearchResults()

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    original_question: str
    current_search_query: str
    search_results: List[str]
    collected_information: List[str]
    is_sufficient: bool
    suggested_queries: List[str]
    final_answer: Optional[str]
    iteration_count: int  # ë°˜ë³µ íšŸìˆ˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ í•„ë“œ ì¶”ê°€

# ê²€ìƒ‰ ì—ì´ì „íŠ¸ (Agent 1)
search_agent_prompt = PromptTemplate.from_template("""
You are a search agent. Your task is to search for information based on the given query.

Search Query: {input}

Use the following tool to search for information:
{tools}

Use the following format:
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the tool
Observation: the result of the tool
Thought: I now have the search results
Final Answer: Summarize the search results in a clear and concise way

{agent_scratchpad}
""")

search_agent = create_react_agent(llm, [search_tool], search_agent_prompt)
search_executor = AgentExecutor(
    agent=search_agent, 
    tools=[search_tool], 
    verbose=True,
    handle_parsing_errors=True  # íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬ ì˜µì…˜ ì¶”ê°€
)

# ê²€ì¦ ì—ì´ì „íŠ¸ (Agent 2)
verification_prompt = PromptTemplate.from_template("""
You are a verification agent. Your task is to determine if the collected information is sufficient to answer the original question.

Original Question: {original_question}
Collected Information:
{collected_information}

First, analyze the original question to identify:
1. The main topic
2. Specific requirements or constraints
3. What information would be needed to provide a comprehensive answer

Then, evaluate the collected information to determine if it addresses all aspects of the question.

First, provide a detailed analysis of the information collected and how it relates to the question.
Then, based on this analysis, determine if the information is sufficient.

Return your analysis in JSON format:
{{
    "verification_reason": "Detailed explanation of your analysis of the collected information in relation to the question",
    "is_sufficient": true/false
}}

Focus on providing a thorough analysis first, then make your determination about sufficiency.
""")

def verification_agent(state):
    collected_info = "\n".join(state["collected_information"])
    
    response = llm.invoke(verification_prompt.format(
        original_question=state["original_question"],
        collected_information=collected_info
    ))
    
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            return {
                "is_sufficient": result.get("is_sufficient", False),
                "verification_reason": result.get("verification_reason", "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            }
        else:
            # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "is_sufficient": False,
                "verification_reason": "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "is_sufficient": False,
            "verification_reason": "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        }

# ì¿¼ë¦¬ ì œì•ˆ ì—ì´ì „íŠ¸ (Agent 3)
query_suggestion_prompt = PromptTemplate.from_template("""
You are a query suggestion agent. Your task is to suggest new search queries to find more information.

Original Question: {original_question}
Current Search Query: {current_search_query}
Collected Information:
{collected_information}

What we still need to know:
1. What aspects of the original question are not yet answered?
2. What specific information is missing?

Suggest 1-3 new search queries in English that would help gather the missing information.
Return your suggestions in JSON format:
{{
    "suggested_queries": ["query1", "query2", "query3"]
}}
""")

def query_suggestion_agent(state):
    collected_info = "\n".join(state["collected_information"])
    response = llm.invoke(query_suggestion_prompt.format(
        original_question=state["original_question"],
        current_search_query=state["current_search_query"],
        collected_information=collected_info
    ))
    
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            return {"suggested_queries": result["suggested_queries"]}
        else:
            # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return {"suggested_queries": [
                f"AMD 5600G FPS in League of Legends", 
                f"League of Legends minimum requirements vs AMD 5600G"
            ]}
    except:
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return {"suggested_queries": [
            f"AMD 5600G FPS in League of Legends", 
            f"League of Legends minimum requirements vs AMD 5600G"
        ]}

# ê¸°ì¡´ ChatMemory í´ë˜ìŠ¤ ëŒ€ì‹  Langchainì˜ Memory ì‚¬ìš©
def get_memory():
    if "langchain_memory" not in st.session_state:
        st.session_state.langchain_memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True,
            input_key="question",
            output_key="answer"
        )
    return st.session_state.langchain_memory

# ìµœì¢… ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸ ì •ì˜
final_answer_prompt = PromptTemplate.from_template("""
You are a helpful AI assistant. Your task is to provide a comprehensive answer to the user's question based on the collected information.

Original Question: {original_question}
Collected Information:
{collected_information}

Please provide a detailed, accurate, and helpful answer based on the collected information. 
Make sure to address all aspects of the question and provide specific details where available.
If the information is insufficient to answer any part of the question, acknowledge this limitation.

Your answer should be well-structured, easy to understand, and directly relevant to the question.
""")

def final_answer_agent(state):
    collected_info = "\n".join(state["collected_information"])
    chat_history = state.get("chat_history", "")
    
    response = llm.invoke(final_answer_prompt.format(
        original_question=state["original_question"],
        collected_information=collected_info,
        chat_history=chat_history
    ))
    
    return {"final_answer": response}

# ê²€ìƒ‰ ë…¸ë“œ
def search_node(state):
    # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
    state["iteration_count"] += 1
    
    result = search_executor.invoke({
        "input": state["current_search_query"],
        "tools": [search_tool],
        "tool_names": ["DuckDuckGoSearchResults"],
        "agent_scratchpad": ""
    })
    
    search_results = result["output"]
    state["search_results"].append(search_results)
    state["collected_information"].append(f"Search for '{state['current_search_query']}': {search_results}")
    
    return state

# ë‹¤ìŒ ì¿¼ë¦¬ ì„ íƒ ë…¸ë“œ
def select_next_query(state):
    if state["suggested_queries"]:
        state["current_search_query"] = state["suggested_queries"][0]
        state["suggested_queries"] = state["suggested_queries"][1:]
    return state

# ë¼ìš°í„° í•¨ìˆ˜
def router(state):
    # ìµœëŒ€ 5ë²ˆ ë°˜ë³µ ì œí•œ ì¶”ê°€
    if state["iteration_count"] >= 5:
        return "generate_answer"
    
    if state["is_sufficient"]:
        return "generate_answer"
    elif state["suggested_queries"]:
        return "select_next_query"
    else:
        return "suggest_queries"

# ê·¸ë˜í”„ ì •ì˜
workflow = StateGraph(AgentState)

# ë…¸ë“œ ì¶”ê°€
workflow.add_node("search", search_node)
workflow.add_node("verify", verification_agent)
workflow.add_node("suggest_queries", query_suggestion_agent)
workflow.add_node("select_next_query", select_next_query)
workflow.add_node("generate_answer", final_answer_agent)

# ì—£ì§€ ì¶”ê°€
workflow.add_edge("search", "verify")
# ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€ - verify ë…¸ë“œì—ì„œ ë¼ìš°í„° í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë…¸ë“œ ê²°ì •
workflow.add_conditional_edges(
    "verify",
    router,
    {
        "generate_answer": "generate_answer",
        "select_next_query": "select_next_query",
        "suggest_queries": "suggest_queries"
    }
)
workflow.add_edge("suggest_queries", "select_next_query")
workflow.add_edge("select_next_query", "search")
workflow.add_edge("generate_answer", END)

# ì‹œì‘ì  ì¶”ê°€
workflow.set_entry_point("search")

# ê·¸ë˜í”„ ì»´íŒŒì¼
graph = workflow.compile()

# ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì—ì´ì „íŠ¸ ê°œì„ 
query_optimization_prompt = PromptTemplate.from_template("""
You are a search query optimization agent. Your task is to convert a user's question into effective search queries.

User Question: {question}

First, analyze the question to identify:
1. Key topics and entities
2. Technical terms
3. Specific requirements or constraints

IMPORTANT: If the question is not in English, translate the key concepts to English for better search results.

Then, create 1-3 effective search queries in English that would help find relevant information.
The queries should be concise, specific, and use appropriate technical terms.

For example:
- If the question is about "AMD 5600g ê·¸ë˜í”½ì¹´ë“œ ì—†ì´ ë¦¬ê·¸ì˜¤ë¸Œë ˆì „ë“œ ì‹¤í–‰", create queries like:
  "AMD 5600G integrated graphics League of Legends performance"
  "Can League of Legends run on AMD 5600G without dedicated GPU"

Return your analysis and queries in JSON format:
{{
    "analysis": "Brief analysis of the question",
    "search_queries": ["query1", "query2", "query3"]
}}
""")

def query_optimization_agent(question):
    response = llm.invoke(query_optimization_prompt.format(question=question))
    
    try:
        # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ì‹œë„
        import re
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(0)
            result = json.loads(json_str)
            return result["search_queries"]
        else:
            # JSONì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return [f"AMD 5600G League of Legends performance", 
                   f"Can League of Legends run on AMD 5600G without dedicated GPU"]
    except Exception as e:
        print(f"Error parsing JSON: {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return [f"AMD 5600G League of Legends performance", 
               f"Can League of Legends run on AMD 5600G without dedicated GPU"]

# ê²€ìƒ‰ ë„êµ¬ ìˆ˜ì • - ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš© ë³´ì¥
def search_with_progress(state):
    state["iteration_count"] += 1
    
    # í˜„ì¬ ì¿¼ë¦¬ê°€ í•œêµ­ì–´ì¸ì§€ í™•ì¸í•˜ê³  ì˜ì–´ë¡œ ë³€í™˜
    current_query = state["current_search_query"]
    if any(ord(char) > 127 for char in current_query):
        english_query = f"AMD 5600G League of Legends performance {state['iteration_count']}"
        log_message = f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬ (ì›ë³¸): {current_query}"
        process_logs.append(log_message)
        with progress_container:
            st.write(log_message)
            
        log_message = f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬ (ì˜ì–´ë¡œ ë³€í™˜): {english_query}"
        process_logs.append(log_message)
        with progress_container:
            st.write(log_message)
            
        current_query = english_query
    else:
        log_message = f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬: {current_query}"
        process_logs.append(log_message)
        with progress_container:
            st.write(log_message)
    
    # ì˜ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹¤í–‰
    result = search_executor.invoke({
        "input": current_query,
        "tools": [search_tool],
        "tool_names": ["DuckDuckGoSearchResults"],
        "agent_scratchpad": ""
    })
    
    search_results = result["output"]
    log_message = f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {search_results[:200]}..."
    process_logs.append(log_message)
    with progress_container:
        st.write(log_message)
    
    state["search_results"].append(search_results)
    state["collected_information"].append(f"Search for '{current_query}': {search_results}")
    
    return state

# ì‹¤í–‰ í•¨ìˆ˜ ìˆ˜ì •
def run_agent_workflow(question, memory=None):
    chat_history = ""
    if memory:
        # Langchain ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
        memory_variables = memory.load_memory_variables({})
        chat_history = memory_variables.get("chat_history", "")
    
    # ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    optimized_queries = query_optimization_agent(question)
    
    # ì˜ì–´ ì¿¼ë¦¬ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
    english_queries = []
    for query in optimized_queries:
        if any(ord(char) > 127 for char in query):  # í•œêµ­ì–´ ë¬¸ì í¬í•¨ ì—¬ë¶€ í™•ì¸
            # ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
            english_queries.append(f"AMD 5600G League of Legends performance")
        else:
            english_queries.append(query)
    
    if not english_queries:
        english_queries = [
            "AMD 5600G League of Legends performance",
            "Can League of Legends run on AMD 5600G without dedicated GPU"
        ]
    
    initial_query = english_queries[0]
    
    initial_state = {
        "original_question": question,
        "current_search_query": initial_query,  # ìµœì í™”ëœ ì²« ë²ˆì§¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
        "search_results": [],
        "collected_information": [],
        "is_sufficient": False,
        "suggested_queries": english_queries[1:] if len(english_queries) > 1 else [],  # ë‚˜ë¨¸ì§€ ì¿¼ë¦¬ë“¤ì„ ì œì•ˆ ì¿¼ë¦¬ë¡œ ì„¤ì •
        "final_answer": None,
        "iteration_count": 0,
        "chat_history": chat_history
    }
    
    result = graph.invoke(initial_state)
    return result["final_answer"]

# Streamlit ì•± ìˆ˜ì •
def main():
    st.title("AI ê²€ìƒ‰ ì–´ì‹œìŠ¤í„´íŠ¸")
    
    # ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
    memory = get_memory()
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "current_question" not in st.session_state:
        st.session_state.current_question = None
    if "process_logs" not in st.session_state:
        st.session_state.process_logs = []
    
    # ì‚¬ì´ë“œë°”ì— ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸° ë²„íŠ¼ ì¶”ê°€
    if st.sidebar.button("ëŒ€í™” ê¸°ë¡ ì§€ìš°ê¸°"):
        memory.clear()
        st.session_state.messages = []
        st.session_state.process_logs = []
        st.rerun()
    
    # ì±„íŒ… ì»¨í…Œì´ë„ˆ ìƒì„± (í•˜ë‹¨ ì—¬ë°± ì¶”ê°€)
    chat_container = st.container()
    
    # ì±„íŒ… ì…ë ¥ì°½ (í•­ìƒ ë§¨ ì•„ë˜ì— ìœ„ì¹˜)
    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    
    # ì±„íŒ… ê¸°ë¡ í‘œì‹œ (ì±„íŒ… ì»¨í…Œì´ë„ˆ ë‚´ë¶€)
    with chat_container:
        st.markdown('<div class="chat-container">', unsafe_allow_html=True)
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])
        st.markdown('</div>', unsafe_allow_html=True)
    
    if user_input and not st.session_state.processing:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
        st.session_state.messages.append({"role": "user", "content": user_input})
        with chat_container:
            with st.chat_message("user"):
                st.write(user_input)
        
        # ì²˜ë¦¬ ì¤‘ ìƒíƒœë¡œ ì„¤ì •
        st.session_state.processing = True
        st.session_state.current_question = user_input
        st.session_state.process_logs = []  # ë¡œê·¸ ì´ˆê¸°í™”
        
        # ì²˜ë¦¬ ì‹œì‘ì„ ìœ„í•œ í˜ì´ì§€ ê°±ì‹ 
        st.rerun()
    
    # ì²˜ë¦¬ ì¤‘ì´ê³  í˜„ì¬ ì§ˆë¬¸ì´ ìˆëŠ” ê²½ìš°
    if st.session_state.processing and st.session_state.current_question:
        with chat_container:
            with st.chat_message("assistant"):
                # ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ
                progress_container = st.container()
                
                with progress_container:
                    # ì§„í–‰ ìƒíƒœ í‘œì‹œ ì»´í¬ë„ŒíŠ¸
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
                    def update_progress(progress, status):
                        progress_bar.progress(progress)
                        status_text.markdown(f"**{status}**")
                        st.session_state.process_logs.append(status)
                        time.sleep(0.1)  # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì§§ì€ ì§€ì—°
                    
                    # ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                    update_progress(0.1, "ğŸ” ì§ˆë¬¸ ë¶„ì„ ë° ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì¤‘...")
                    
                    optimized_queries = query_optimization_agent(st.session_state.current_question)
                    
                    # ì˜ì–´ ì¿¼ë¦¬ì¸ì§€ í™•ì¸í•˜ê³ , ì•„ë‹ˆë©´ ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
                    english_queries = []
                    for query in optimized_queries:
                        if any(ord(char) > 127 for char in query):  # í•œêµ­ì–´ ë¬¸ì í¬í•¨ ì—¬ë¶€ í™•ì¸
                            # ê¸°ë³¸ ì˜ì–´ ì¿¼ë¦¬ë¡œ ëŒ€ì²´
                            english_queries.append(f"AMD 5600G League of Legends performance")
                        else:
                            english_queries.append(query)
                    
                    if not english_queries:
                        english_queries = [
                            "AMD 5600G League of Legends performance",
                            "Can League of Legends run on AMD 5600G without dedicated GPU"
                        ]
                    
                    update_progress(0.2, f"ğŸ“ ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬: {', '.join(english_queries)}")
                    
                    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                    memory_variables = memory.load_memory_variables({})
                    chat_history = memory_variables.get("chat_history", "")
                    
                    initial_state = {
                        "original_question": st.session_state.current_question,
                        "current_search_query": english_queries[0],  # ìµœì í™”ëœ ì²« ë²ˆì§¸ ì˜ì–´ ì¿¼ë¦¬ ì‚¬ìš©
                        "search_results": [],
                        "collected_information": [],
                        "is_sufficient": False,
                        "suggested_queries": english_queries[1:] if len(english_queries) > 1 else [],  # ë‚˜ë¨¸ì§€ ì¿¼ë¦¬ë“¤ì„ ì œì•ˆ ì¿¼ë¦¬ë¡œ ì„¤ì •
                        "final_answer": None,
                        "iteration_count": 0,
                        "chat_history": chat_history
                    }
                    
                    # ê²€ìƒ‰ ì‹¤í–‰
                    sufficient_info = False
                    for i, query in enumerate(english_queries[:min(5, len(english_queries))]):
                        progress_value = 0.2 + (i * 0.15)  # ê° ê²€ìƒ‰ë§ˆë‹¤ ì§„í–‰ë¥  ì¦ê°€
                        
                        update_progress(progress_value, f"ğŸ“Š ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘: {query}")
                        
                        # ê²€ìƒ‰ ì‹¤í–‰
                        result = search_executor.invoke({
                            "input": query,
                            "tools": [search_tool],
                            "tool_names": ["DuckDuckGoSearchResults"],
                            "agent_scratchpad": ""
                        })
                        
                        search_results = result["output"]
                        truncated_results = search_results[:200] + "..." if len(search_results) > 200 else search_results
                        
                        update_progress(progress_value + 0.05, f"ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ì¤‘...")
                        st.session_state.process_logs.append(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {truncated_results}")
                        
                        initial_state["search_results"].append(search_results)
                        initial_state["collected_information"].append(f"Search for '{query}': {search_results}")
                        
                        # ê²€ì¦ ë‹¨ê³„
                        update_progress(progress_value + 0.1, "âœ… ì •ë³´ ê²€ì¦ ì¤‘...")
                        
                        # ê²€ì¦ ì—ì´ì „íŠ¸ í˜¸ì¶œ
                        collected_info = "\n".join(initial_state["collected_information"])
                        verification_response = llm.invoke(verification_prompt.format(
                            original_question=initial_state["original_question"],
                            collected_information=collected_info
                        ))
                        
                        try:
                            # ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                            import re
                            json_match = re.search(r'\{.*\}', verification_response, re.DOTALL)
                            if json_match:
                                json_str = json_match.group(0)
                                verification_result = json.loads(json_str)
                                verification_reason = verification_result.get("verification_reason", "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                is_sufficient = verification_result.get("is_sufficient", False)
                            else:
                                verification_reason = "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                is_sufficient = False
                        except Exception as e:
                            print(f"Error parsing JSON: {e}")
                            verification_reason = "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            is_sufficient = False
                        
                        # ê²€ì¦ ê²°ê³¼ ë¡œê·¸ì— ì¶”ê°€
                        st.session_state.process_logs.append(f"ğŸ’¡ ê²€ì¦ ë¶„ì„: {verification_reason}")
                        
                        if is_sufficient:
                            update_progress(progress_value + 0.15, "ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ë¨ âœ…")
                            st.session_state.process_logs.append("ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶©ë¶„í•œ ì •ë³´ ìˆ˜ì§‘ë¨ âœ…")
                            sufficient_info = True
                            break
                        else:
                            update_progress(progress_value + 0.15, "ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶”ê°€ ì •ë³´ í•„ìš” âŒ")
                            st.session_state.process_logs.append("ğŸ“‹ ê²€ì¦ ê²°ê³¼: ì¶”ê°€ ì •ë³´ í•„ìš” âŒ")
                    
                    # ìµœì¢… ë‹µë³€ ìƒì„±
                    update_progress(0.9, "ğŸ“ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
                    
                    final_answer_response = llm.invoke(final_answer_prompt.format(
                        original_question=initial_state["original_question"],
                        collected_information="\n".join(initial_state["collected_information"])
                    ))
                    answer = final_answer_response
                    
                    # ì§„í–‰ ì™„ë£Œ
                    update_progress(1.0, "âœ¨ ë‹µë³€ ìƒì„± ì™„ë£Œ!")
                    time.sleep(0.5)  # ì™„ë£Œ ë©”ì‹œì§€ë¥¼ ì ì‹œ í‘œì‹œ
                    
                    # ì§„í–‰ ìƒí™© ì»¨í…Œì´ë„ˆ ë¹„ìš°ê¸°
                    progress_container.empty()
                
                # ìµœì¢… ë‹µë³€ í‘œì‹œ (ì¹´ë“œ í˜•íƒœë¡œ)
                # st.markdown("""
                # <div style="background-color: #f0f7ff; padding: 1.5rem; border-radius: 10px; box-shadow: 0 2px 5px rgba(0,0,0,0.1);">
                # """, unsafe_allow_html=True)
                st.write(answer)
                # st.markdown("</div>", unsafe_allow_html=True)
                
                # ìƒì„± ê³¼ì •ì„ í† ê¸€ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ
                with st.expander("ğŸ“Š ë‹µë³€ ìƒì„± ê³¼ì • ë³´ê¸°", expanded=False):
                    for log in st.session_state.process_logs:
                        st.write(log)
                
                # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
                memory.save_context({"question": st.session_state.current_question}, {"answer": answer})
                
                # ë©”ì‹œì§€ ì €ì¥
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # ì²˜ë¦¬ ì™„ë£Œ ìƒíƒœë¡œ ì„¤ì •
                st.session_state.processing = False
                st.session_state.current_question = None

if __name__ == "__main__":
    main()
