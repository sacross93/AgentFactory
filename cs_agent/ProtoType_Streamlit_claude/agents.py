from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import AgentExecutor, create_react_agent
from langgraph.graph import StateGraph, END
from typing import TypedDict, List, Dict, Any, Optional
import json
import re
from logging_config import get_logger
import time
import requests
from langchain_community.utilities import DuckDuckGoSearchAPIWrapper
from langchain_community.tools import BaseTool

# ìƒíƒœ ì •ì˜
class AgentState(TypedDict):
    original_question: str
    current_search_query: str
    search_results: List[str]
    collected_information: List[str]
    is_sufficient: bool
    suggested_queries: List[str]
    final_answer: Optional[str]
    iteration_count: int
    chat_history: str

# í”„ë¡¬í”„íŠ¸ ì •ì˜
search_agent_prompt = PromptTemplate.from_template("""
You are a search agent. Your task is to search for information based on the given query.

Search Query: {input}

Use the following tool to search for information:
{tools}

Use the following format:
Thought: you should always think about what to do
Action: the action to take, should be one of [{tool_names}]
Action Input: the input to the tool (just the search query text, no additional formatting)
Observation: the result of the tool
Thought: I now have the search results
Final Answer: Summarize the search results in a clear and concise way

{agent_scratchpad}
""")

verification_prompt = PromptTemplate.from_template("""
You are a verification agent. Your task is to determine if the collected information is sufficient to answer the original question.

Original Question: {original_question}
Collected Information:
{collected_information}

First, analyze the original question to identify:
1. The main topic
2. Specific requirements or constraints
3. What information would be needed to provide a comprehensive answer

Then, evaluate the collected information to determine if it addresses all aspects of the question.

First, provide a detailed analysis of the information collected and how it relates to the question.
Then, based on this analysis, determine if the information is sufficient.

Return your analysis in JSON format:
{{
    "verification_reason": "Detailed explanation of your analysis of the collected information in relation to the question",
    "is_sufficient": true/false
}}

Focus on providing a thorough analysis first, then make your determination about sufficiency.
""")

query_suggestion_prompt = PromptTemplate.from_template("""
You are a query suggestion agent. Your task is to suggest new search queries to find more information.

Original Question: {original_question}
Current Search Query: {current_search_query}
Collected Information:
{collected_information}

What we still need to know:
1. What aspects of the original question are not yet answered?
2. What specific information is missing?

Suggest 1-3 new search queries in English that would help gather the missing information.
Return your suggestions in JSON format:
{{
    "suggested_queries": ["query1", "query2", "query3"]
}}
""")

final_answer_prompt = PromptTemplate.from_template("""
You are a helpful AI assistant. Your task is to provide a comprehensive answer to the user's question based on the collected information.

Original Question: {original_question}
Collected Information:
{collected_information}

Chat History:
{chat_history}

Please provide a detailed, accurate, and helpful answer based on the collected information and chat history. 
Make sure to address all aspects of the question and provide specific details where available.
If the information is insufficient to answer any part of the question, acknowledge this limitation.

If this is a follow-up question to a previous conversation, make sure to consider the context of the previous messages.
For example, if the user previously asked about hardware requirements and now asks "what problems might occur?", 
you should understand they're referring to problems with the hardware discussed earlier.

Your answer should be well-structured, easy to understand, and directly relevant to the question.

IMPORTANT: Your final answer MUST be in Korean language, regardless of the language of the question or search results.
""")

translation_prompt = PromptTemplate.from_template("""
You are a professional translator. Your task is to translate the given text into Korean.
The text might already contain some Korean, but ensure the entire response is in fluent, natural Korean.

Original text:
{text}

Please translate this text into Korean, maintaining the original meaning, tone, and technical accuracy.
If the text already contains Korean, make sure the entire response is in consistent, high-quality Korean.
""")

query_optimization_prompt = PromptTemplate.from_template("""
You are a search query optimization agent. Your task is to convert a user's question into effective search queries.

User Question: {question}
Chat History:
{chat_history}

First, analyze the question and chat history to identify:
1. Key topics and entities
2. Technical terms
3. Specific requirements or constraints
4. Context from previous conversation

IMPORTANT: 
- If this is a follow-up question, use the context from the chat history to create more specific queries.
- If the question is not in English, translate the key concepts to English for better search results.

Then, create 1-3 effective search queries in English that would help find relevant information.
The queries should be concise, specific, and use appropriate technical terms.

For example:
- If the previous conversation was about "AMD 5600g without GPU for League of Legends" and the new question is 
  "what problems might occur?", create queries like:
  "AMD 5600G integrated graphics problems running games"
  "Issues playing games without dedicated GPU on AMD 5600G"

Return your analysis and queries in JSON format:
{{
    "analysis": "Brief analysis of the question and its context",
    "search_queries": ["query1", "query2", "query3"]
}}
""")

# ëª¨ë“ˆë³„ ë¡œê±° ê°€ì ¸ì˜¤ê¸°
logger = get_logger("AgentSystem")

# ê²€ìƒ‰ í•¨ìˆ˜ ìˆ˜ì • - Serper API ì‚¬ìš©
def web_search(query, num_results=5):
    """ê²€ìƒ‰ ìˆ˜í–‰ í•¨ìˆ˜"""
    try:
        # Serper APIë‚˜ SerpAPI ëŒ€ì‹  ê°„ë‹¨í•œ ê²€ìƒ‰ ì—ë®¬ë ˆì´ì…˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
        # ì‹¤ì œ ë°°í¬ ì‹œ ì•„ë˜ ì£¼ì„ ì²˜ë¦¬ëœ ì½”ë“œ í™œì„±í™” í•„ìš”
        
        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
        results = [
            {
                "title": f"ê²€ìƒ‰ ê²°ê³¼ 1: {query}",
                "link": "https://example.com/result1",
                "snippet": f"{query}ì— ê´€í•œ ì²« ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì´ ê²°ê³¼ëŠ” ê´€ë ¨ëœ ì¤‘ìš” ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤."
            },
            {
                "title": f"ê²€ìƒ‰ ê²°ê³¼ 2: {query}",
                "link": "https://example.com/result2",
                "snippet": f"{query}ì— ê´€í•œ ë‘ ë²ˆì§¸ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ìì„¸í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            }
        ]
        
        # íŠ¹ì • í‚¤ì›Œë“œì— ëŒ€í•œ ì‹¤ì œ ì‘ë‹µ ì‹œë®¬ë ˆì´ì…˜
        if "ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œ" in query or "league of legends" in query.lower():
            results = [
                {
                    "title": "League of Legends ì‹œìŠ¤í…œ ìš”êµ¬ ì‚¬í•­ - Riot Games",
                    "link": "https://support-leagueoflegends.riotgames.com/hc/ko/articles/201752654",
                    "snippet": "ìµœì†Œ ì‚¬ì–‘: CPU: Intel Core i3-530 ë˜ëŠ” AMD A6-3650, RAM: 4GB, ê·¸ë˜í”½ ì¹´ë“œ: NVIDIA GeForce 9600GT. ê¶Œì¥ ì‚¬ì–‘: CPU: Intel Core i5-3300 ë˜ëŠ” AMD Ryzen 3, RAM: 8GB, ê·¸ë˜í”½ ì¹´ë“œ: NVIDIA GeForce GTX 660."
                },
                {
                    "title": "ë¦¬ê·¸ ì˜¤ë¸Œ ë ˆì „ë“œ ìµœì†Œ ë° ê¶Œì¥ ì‹œìŠ¤í…œ ì‚¬ì–‘",
                    "link": "https://www.leagueoflegends.com/ko-kr/news/game-updates/system-requirements/",
                    "snippet": "ê¶Œì¥ ì‚¬ì–‘: ìš´ì˜ì²´ì œ: Windows 10 64ë¹„íŠ¸, CPU: Intel Core i5 ë˜ëŠ” AMD Ryzen 5, ë©”ëª¨ë¦¬: 8GB RAM, ê·¸ë˜í”½: NVIDIA GeForce GTX 660 ë˜ëŠ” AMD Radeon HD 7870."
                }
            ]
        
        logger.info(f"ê²€ìƒ‰ ì„±ê³µ: '{query}', ê²°ê³¼ {len(results)}ê°œ ë°˜í™˜")
        return results
    
    except Exception as e:
        logger.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return []

class AgentSystem:
    def __init__(self, llm=None):
        self.llm = llm
        # ì§ì ‘ êµ¬í˜„í•œ ê²€ìƒ‰ í•¨ìˆ˜ ì‚¬ìš©
        self.search_function = web_search
        # ì—ì´ì „íŠ¸ ìƒì„± ë¶€ë¶„ ì œê±° (ì§ì ‘ ê²€ìƒ‰ í•¨ìˆ˜ ì‚¬ìš©)
        # í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤ ì†ì„±ìœ¼ë¡œ ì¶”ê°€
        self.final_answer_prompt = final_answer_prompt
        self.graph = self._create_workflow()
        logger.info("ê¸°ë³¸ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _create_workflow(self):
        logger.info("ì›¹ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì‹œì‘")
        # ê·¸ë˜í”„ ì •ì˜
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("search", self.search_node)
        workflow.add_node("verify", self.verification_agent)
        workflow.add_node("suggest_queries", self.query_suggestion_agent)
        workflow.add_node("select_next_query", self.select_next_query)
        workflow.add_node("generate_answer", self.final_answer_agent)
        workflow.add_node("query_optimization", self.query_optimization_node)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.add_edge("search", "verify")
        workflow.add_conditional_edges(
            "verify",
            self.router,
            {
                "generate_answer": "generate_answer",
                "select_next_query": "select_next_query",
                "suggest_queries": "suggest_queries"
            }
        )
        workflow.add_edge("suggest_queries", "select_next_query")
        workflow.add_edge("select_next_query", "search")
        workflow.add_edge("generate_answer", END)
        workflow.add_edge("query_optimization", "select_next_query")
        
        # ì‹œì‘ì  ì¶”ê°€
        workflow.set_entry_point("search")
        
        # ê·¸ë˜í”„ ì»´íŒŒì¼
        return workflow.compile()
    
    def search_node(self, state):
        """ê²€ìƒ‰ ë…¸ë“œ - ìƒì„¸ ë¡œê·¸ ì¶”ê°€"""
        # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€
        state["iteration_count"] += 1
        
        # ê²€ìƒ‰ ì‹œì‘ ì‹œê°„ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
        search_start_time = time.time()
        timestamp = time.strftime('%H:%M:%S')
        logger.info(f"[{timestamp}] ê²€ìƒ‰ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œì‘ ({state['iteration_count']}ì°¨): '{state['current_search_query']}'")
        
        # ê²€ìƒ‰ ê³¼ì • ë¡œê·¸ì— ì¶”ê°€ (íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨)
        state["collected_information"].append(
            f"ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ ({state['iteration_count']}ì°¨): '{state['current_search_query']}' ({timestamp})"
        )
        
        try:
            # ì§„í–‰ ìƒíƒœ ì¶”ê°€
            state["collected_information"].append(f"â³ ê²€ìƒ‰ ì¤‘... ({timestamp})")
            
            # ì§ì ‘ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ
            search_results_list = self.search_function(state["current_search_query"])
            
            # ê²€ìƒ‰ ì™„ë£Œ ì‹œê°„ ë° ì†Œìš” ì‹œê°„ ê¸°ë¡
            search_end_time = time.time()
            search_duration = search_end_time - search_start_time
            search_end_timestamp = time.strftime('%H:%M:%S')
            logger.info(f"[{search_end_timestamp}] ê²€ìƒ‰ ì™„ë£Œ: {len(search_results_list)}ê°œ ê²°ê³¼, ì†Œìš” ì‹œê°„: {search_duration:.2f}ì´ˆ")
            
            # ì†Œìš” ì‹œê°„ ë¡œê·¸ ì¶”ê°€
            state["collected_information"].append(
                f"â±ï¸ ê²€ìƒ‰ ì™„ë£Œ: ì†Œìš” ì‹œê°„ {search_duration:.2f}ì´ˆ ({search_end_timestamp})"
            )
            
            # ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            if not search_results_list:
                state["collected_information"].append("â— ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return state
            
            # ê²°ê³¼ ê°œìˆ˜ ë¡œê·¸ ì¶”ê°€
            state["collected_information"].append(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(search_results_list)}ê°œ í•­ëª© ë°œê²¬")
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in search_results_list:
                formatted_results.append(
                    f"Title: {result['title']}\nLink: {result['link']}\nSnippet: {result['snippet']}\n"
                )
            
            search_results = "\n".join(formatted_results)
            
            # ê²°ê³¼ ì €ì¥
            state["search_results"].append(search_results)
            
            # ê²€ìƒ‰ ê²°ê³¼ ë¡œê·¸ ì¶”ê°€ - ê° ê²°ê³¼ë¥¼ ë³„ë„ë¡œ ê¸°ë¡
            for i, result in enumerate(search_results_list):
                # ì œëª©ê³¼ ë‚´ìš© ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ (êµ¬ë¶„ì„  ì¶”ê°€)
                state["collected_information"].append(
                    f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ {i+1}: {result['title']}"
                )
                state["collected_information"].append(
                    f"   {result['snippet'][:200]}..."
                )
                state["collected_information"].append(
                    f"   ğŸ”— ì¶œì²˜: {result['link']}"
                )
                # êµ¬ë¶„ì„  ì¶”ê°€ (ë§ˆì§€ë§‰ í•­ëª© ì œì™¸)
                if i < len(search_results_list) - 1:
                    state["collected_information"].append("   ---")
            
            return state
            
        except Exception as e:
            error_timestamp = time.strftime('%H:%M:%S')
            error_msg = str(e)
            logger.error(f"[{error_timestamp}] ê²€ìƒ‰ ì˜¤ë¥˜: {error_msg}")
            state["collected_information"].append(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜ ({error_timestamp}): {error_msg}")
            return state
    
    def verification_agent(self, state):
        collected_info = "\n".join(state["collected_information"])
        
        response = self.llm.invoke(verification_prompt.format(
            original_question=state["original_question"],
            collected_information=collected_info
        ))
        
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                return {
                    "is_sufficient": result.get("is_sufficient", False),
                    "verification_reason": result.get("verification_reason", "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                }
            else:
                return {
                    "is_sufficient": False,
                    "verification_reason": "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
        except Exception as e:
            print(f"Error parsing JSON: {e}")
            return {
                "is_sufficient": False,
                "verification_reason": "ì •ë³´ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }
    
    def query_suggestion_agent(self, state):
        collected_info = "\n".join(state["collected_information"])
        response = self.llm.invoke(query_suggestion_prompt.format(
            original_question=state["original_question"],
            current_search_query=state["current_search_query"],
            collected_information=collected_info
        ))
        
        try:
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)
                return {"suggested_queries": result["suggested_queries"]}
            else:
                return {"suggested_queries": [
                    f"AMD 5600G FPS in League of Legends", 
                    f"League of Legends minimum requirements vs AMD 5600G"
                ]}
        except:
            return {"suggested_queries": [
                f"AMD 5600G FPS in League of Legends", 
                f"League of Legends minimum requirements vs AMD 5600G"
            ]}
    
    def select_next_query(self, state):
        if state["suggested_queries"]:
            state["current_search_query"] = state["suggested_queries"][0]
            state["suggested_queries"] = state["suggested_queries"][1:]
        return state
    
    def router(self, state):
        if state["iteration_count"] >= 5:
            return "generate_answer"
        
        if state["is_sufficient"]:
            return "generate_answer"
        elif state["suggested_queries"]:
            return "select_next_query"
        else:
            return "suggest_queries"
    
    def final_answer_agent(self, state):
        collected_info = "\n".join(state["collected_information"])
        chat_history = state.get("chat_history", "")
        
        # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¶Œì¥ ì‚¬ì–‘ ì •ë³´ ê°•ì¡°
        enhanced_prompt = final_answer_prompt.format(
            original_question=state["original_question"],
            collected_information=collected_info,
            chat_history=chat_history
        ) + "\n\nì¤‘ìš”: ê²€ìƒ‰ ê²°ê³¼ì— 'ìµœì†Œ ì‚¬ì–‘(Minimum Specs)'ê³¼ 'ê¶Œì¥ ì‚¬ì–‘(Recommended Specs)'ì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´, ë‘ ê°€ì§€ë¥¼ ëª¨ë‘ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ë‹µë³€ì— í¬í•¨ì‹œì¼œ ì£¼ì„¸ìš”."
        
        response = self.llm.invoke(enhanced_prompt)
        
        # í•œê¸€ ì‘ë‹µ ì²´í¬ ë° ë²ˆì—­
        is_korean, needs_translation = self.check_korean_response(response)
        
        # ë²ˆì—­ì´ í•„ìš”í•œ ê²½ìš°
        if needs_translation:
            translation_prompt_enhanced = """
            ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. íŠ¹íˆ ê¸°ìˆ  ìš©ì–´ì™€ ì‚¬ì–‘ ì •ë³´ëŠ” ì •í™•í•˜ê²Œ ë²ˆì—­í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
            
            ì›ë¬¸:
            {text}
            
            ë²ˆì—­ ì‹œ ì£¼ì˜ì‚¬í•­:
            1. 'Minimum Specs'ëŠ” 'ìµœì†Œ ì‚¬ì–‘'ìœ¼ë¡œ ë²ˆì—­
            2. 'Recommended Specs'ëŠ” 'ê¶Œì¥ ì‚¬ì–‘'ìœ¼ë¡œ ë²ˆì—­
            3. ëª¨ë“  í•˜ë“œì›¨ì–´ ì‚¬ì–‘ ì •ë³´(CPU, RAM, ê·¸ë˜í”½ ì¹´ë“œ ë“±)ëŠ” ëˆ„ë½ ì—†ì´ ë²ˆì—­
            """
            
            translated_response = self.llm.invoke(translation_prompt_enhanced.format(text=response))
            return {"final_answer": translated_response, "was_translated": True}
        
        return {"final_answer": response, "was_translated": False}
    
    def translate_to_korean(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ í•œê¸€ë¡œ ë²ˆì—­í•˜ëŠ” í•¨ìˆ˜"""
        response = self.llm.invoke(translation_prompt.format(text=text))
        return response
    
    def check_korean_response(self, response):
        """
        ì‘ë‹µì´ ì ì ˆí•œ í•œê¸€ë¡œ ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
        1. ì¤‘êµ­ì–´ê°€ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜
        2. í•œê¸€ì´ í•œ ê°œë„ ì—†ìœ¼ë©´
        ë²ˆì—­ì´ í•„ìš”í•˜ë‹¤ê³  íŒë‹¨
        """
        # ì¤‘êµ­ì–´ ë¬¸ì ë²”ìœ„ ì²´í¬ (ê°„ì²´ ë° ë²ˆì²´)
        has_chinese = any(0x4E00 <= ord(char) <= 0x9FFF for char in response)
        
        # í•œê¸€ ë¬¸ì ì²´í¬
        has_korean = any(0xAC00 <= ord(char) <= 0xD7A3 for char in response)
        
        # ì¤‘êµ­ì–´ê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜ í•œê¸€ì´ ì—†ìœ¼ë©´ ë²ˆì—­ í•„ìš”
        needs_translation = has_chinese or not has_korean
        
        return not needs_translation, needs_translation
    
    def query_optimization_node(self, state):
        """ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ë…¸ë“œ"""
        logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì‹œì‘: {state['original_question']}")
        
        try:
            response = self.llm.invoke(query_optimization_prompt.format(
                question=state["original_question"],
                chat_history=state["chat_history"]
            ))
            
            # ë¡œê·¸ì— ì¶”ê°€
            state["collected_information"].append(f"ğŸ” ì›ë³¸ ì§ˆë¬¸: '{state['original_question']}'")
            
            # JSON ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group(0))
                search_queries = result.get("search_queries", [])
                
                if search_queries:
                    state["current_search_query"] = search_queries[0]
                    state["suggested_queries"] = search_queries[1:] if len(search_queries) > 1 else []
                    
                    # ë¡œê·¸ì— ìµœì í™”ëœ ì¿¼ë¦¬ ì¶”ê°€
                    state["collected_information"].append(f"ğŸ” ìµœì í™”ëœ ê²€ìƒ‰ ì¿¼ë¦¬: '{state['current_search_query']}'")
                    if state["suggested_queries"]:
                        state["collected_information"].append(f"ğŸ“‹ ì¶”ê°€ ê²€ìƒ‰ ì¿¼ë¦¬ í›„ë³´: {', '.join(state['suggested_queries'])}")
                else:
                    # ê¸°ë³¸ ì¿¼ë¦¬ ì„¤ì •
                    state["current_search_query"] = state["original_question"]
                    state["collected_information"].append(f"âš ï¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì§ˆë¬¸ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©: '{state['current_search_query']}'")
            else:
                # ê¸°ë³¸ ì¿¼ë¦¬ ì„¤ì •
                state["current_search_query"] = state["original_question"]
                state["collected_information"].append(f"âš ï¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì§ˆë¬¸ì„ ì¿¼ë¦¬ë¡œ ì‚¬ìš©: '{state['current_search_query']}'")
            
            return state
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ìµœì í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            state["current_search_query"] = state["original_question"]
            state["collected_information"].append(f"âŒ ì¿¼ë¦¬ ìµœì í™” ì˜¤ë¥˜: {str(e)}")
            return state
    
    def final_answer_node(self, state):
        """ìµœì¢… ë‹µë³€ ìƒì„± ë…¸ë“œ"""
        logger.info("ìµœì¢… ë‹µë³€ ìƒì„± ì‹œì‘")
        
        # ìˆ˜ì§‘ëœ ì •ë³´ í†µí•©
        collected_info = "\n".join(state["collected_information"])
        
        # ë¡œê·¸ì— ì¶”ê°€
        state["collected_information"].append("ğŸ“ ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
        
        try:
            response = self.llm.invoke(final_answer_prompt.format(
                original_question=state["original_question"],
                collected_information=collected_info
            ))
            
            state["final_answer"] = response
            logger.info("ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
            
            # ë¡œê·¸ì— ì¶”ê°€ (ë‹µë³€ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½)
            answer_summary = response[:100] + "..." if len(response) > 100 else response
            state["collected_information"].append(f"âœ… ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ: {answer_summary}")
            
            return state
        except Exception as e:
            logger.error(f"ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            state["final_answer"] = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            state["collected_information"].append(f"âŒ ìµœì¢… ë‹µë³€ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return state
    
    def run_workflow(self, question, chat_history=""):
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        logger.info(f"ì›Œí¬í”Œë¡œìš° ì‹œì‘: {question}")
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = AgentState(
            original_question=question,
            current_search_query="",
            search_results=[],
            collected_information=[],
            is_sufficient=False,
            suggested_queries=[],
            final_answer=None,
            iteration_count=0,
            chat_history=chat_history
        )
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        final_state = self.graph.invoke(initial_state)
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸ ì¶”ê°€
        logger.info(f"ìˆ˜ì§‘ëœ ì •ë³´ ê°œìˆ˜: {len(final_state['collected_information'])}")
        for i, info in enumerate(final_state['collected_information']):
            logger.info(f"ì •ë³´ {i+1}: {info[:100]}...")
        
        # ê²°ê³¼ ì²˜ë¦¬ ì „ì— ì¤‘ë³µ ê²€ì‚¬
        if "final_answer" in final_state and final_state["final_answer"]:
            # ì¤‘ë³µ ê²€ì‚¬ ë° ì œê±°
            original_answer = final_state["final_answer"]
            cleaned_answer = self._remove_duplicates(original_answer)
            
            # ì¤‘ë³µì´ ê°ì§€ë˜ë©´ ë¡œê·¸ ê¸°ë¡
            if len(cleaned_answer) < len(original_answer):
                logger.warning("ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ ì¤‘ë³µ ë‚´ìš© ê°ì§€ ë° ì œê±°")
                final_state["final_answer"] = cleaned_answer
        
        # ê²°ê³¼ ë°˜í™˜ - ìˆ˜ì§‘ëœ ì •ë³´ì™€ ì›ì‹œ ê²€ìƒ‰ ê²°ê³¼ í¬í•¨
        return {
            "final_answer": final_state["final_answer"],
            "collected_information": final_state["collected_information"],
            "raw_search_results": final_state["search_results"]  # ì›ì‹œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        }

    def _remove_duplicates(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ë³µëœ ë¶€ë¶„ ì œê±°"""
        # 1. ì™„ì „íˆ ë™ì¼í•œ ë‘ ë¶€ë¶„ ì²˜ë¦¬
        if len(text) % 2 == 0:
            half = len(text) // 2
            if text[:half] == text[half:]:
                return text[:half]
        
        # 2. ë‹¨ë½ ë‹¨ìœ„ ì¤‘ë³µ ì²˜ë¦¬
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        unique_paragraphs = []
        seen = set()
        
        for para in paragraphs:
            # ì§€ë¬¸ ìƒì„± - ê°„ë‹¨í•œ í•´ì‹œ ëŒ€ì‹  í…ìŠ¤íŠ¸ ìì²´ë¥¼ ì‚¬ìš©
            if para not in seen:
                unique_paragraphs.append(para)
                seen.add(para)
        
        # ì¤‘ë³µì´ ìˆì—ˆìœ¼ë©´ ì¤‘ë³µ ì œê±°í•œ ë²„ì „ ë°˜í™˜
        if len(unique_paragraphs) < len(paragraphs):
            return '\n\n'.join(unique_paragraphs)
        
        # ì¤‘ë³µì´ ì—†ì—ˆìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        return text